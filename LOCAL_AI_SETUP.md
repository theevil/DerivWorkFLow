# Configuraci√≥n de IA Local para DerivWorkFlow

Este documento explica c√≥mo configurar y usar la IA local en DerivWorkFlow para evitar depender de APIs externas como OpenAI.

## üöÄ Caracter√≠sticas

- **IA Local Completa**: An√°lisis de mercado, decisiones de trading y gesti√≥n de riesgos usando modelos locales
- **Sin Costos Externos**: No m√°s pagos por tokens de OpenAI
- **Privacidad Total**: Todos los datos se procesan localmente
- **Optimizado para Microtransacciones**: Modelos peque√±os y r√°pidos ideales para trading de alta frecuencia
- **Fallback Inteligente**: Si la IA local falla, puede usar OpenAI como respaldo

## üìã Requisitos

### Hardware M√≠nimo
- **CPU**: Intel i5/AMD Ryzen 5 o superior
- **RAM**: 8GB m√≠nimo, 16GB recomendado
- **GPU**: Opcional pero recomendado (NVIDIA con 4GB+ VRAM)
- **Almacenamiento**: 10GB libres para modelos

### Software
- Docker y Docker Compose
- Ollama (incluido en el docker-compose)
- Conexi√≥n a internet para descargar modelos inicialmente

## üõ†Ô∏è Instalaci√≥n

### 1. Clonar el Repositorio
```bash
git clone <repository-url>
cd DerivWorkFlow
```

### 2. Configurar Variables de Entorno
```bash
# Copiar el archivo de ejemplo
cp backend-env-example.txt apps/backend/.env

# Editar la configuraci√≥n
nano apps/backend/.env
```

Configuraci√≥n recomendada para IA local:
```env
# Local AI Configuration
LOCAL_AI_ENABLED=true
OLLAMA_HOST=http://localhost:11434
DEFAULT_AI_MODEL=phi-3-mini
LOCAL_AI_FALLBACK_ENABLED=true
LOCAL_AI_TIMEOUT=30
LOCAL_AI_MAX_RETRIES=3
AI_PROVIDER=local  # local, openai, hybrid
```

### 3. Iniciar con Docker Compose
```bash
# Iniciar todos los servicios incluyendo Ollama
docker-compose up -d

# Verificar que Ollama est√© funcionando
docker-compose logs ollama
```

### 4. Inicializar Modelos de IA
```bash
# Ejecutar el script de inicializaci√≥n
docker-compose exec backend python scripts/init_local_ai.py
```

O manualmente:
```bash
# Conectar a Ollama
docker-compose exec ollama ollama pull phi3:mini
docker-compose exec ollama ollama pull gemma2:2b
docker-compose exec ollama ollama pull llama3.1:3b
```

## üéØ Modelos Disponibles

### Modelos Recomendados para Microtransacciones

| Modelo | Tama√±o | Velocidad | Memoria | Uso Recomendado |
|--------|--------|-----------|---------|-----------------|
| **Phi-3 Mini** | 3.8B | ‚ö°‚ö°‚ö°‚ö°‚ö° | 4GB | **Recomendado** - R√°pido y eficiente |
| **Gemma 2B** | 2B | ‚ö°‚ö°‚ö°‚ö°‚ö° | 3GB | Muy r√°pido, menor precisi√≥n |
| **Llama 3.1 3B** | 3B | ‚ö°‚ö°‚ö°‚ö° | 4GB | Buen balance velocidad/precisi√≥n |
| **Llama 3.1 8B** | 8B | ‚ö°‚ö°‚ö° | 8GB | Mayor precisi√≥n, m√°s lento |
| **Mistral 7B** | 7B | ‚ö°‚ö°‚ö° | 7GB | Excelente precisi√≥n |

### Modelos OpenAI (Fallback)
- GPT-4o Mini
- GPT-4o
- GPT-4 Turbo
- GPT-3.5 Turbo

## ‚öôÔ∏è Configuraci√≥n en la Interfaz

### 1. Acceder a Configuraci√≥n
1. Inicia sesi√≥n en la aplicaci√≥n
2. Ve a **Settings** ‚Üí **AI Configuration**

### 2. Seleccionar Proveedor de IA
- **Local AI (Recomendado)**: Usa solo modelos locales
- **OpenAI**: Usa solo OpenAI (requiere API key)
- **Hybrid**: Combina ambos (local como principal, OpenAI como fallback)

### 3. Configurar Modelo Local
- Selecciona el modelo preferido (Phi-3 Mini recomendado)
- Ajusta la temperatura (0.1-0.3 para trading)
- Configura el umbral de confianza (0.6-0.8)

### 4. Probar Conexi√≥n
- Haz clic en **"Test Local AI"** para verificar que los modelos funcionan
- Haz clic en **"Initialize Models"** si necesitas descargar modelos

## üîß Configuraci√≥n Avanzada

### Variables de Entorno Adicionales

```env
# Configuraci√≥n de Rendimiento
LOCAL_AI_TIMEOUT=30          # Timeout en segundos
LOCAL_AI_MAX_RETRIES=3       # Intentos m√°ximos
LOCAL_AI_FALLBACK_ENABLED=true  # Habilitar fallback

# Configuraci√≥n de Modelos
DEFAULT_AI_MODEL=phi-3-mini  # Modelo por defecto
AI_PROVIDER=local            # Proveedor preferido
```

### Configuraci√≥n de GPU (Opcional)

Si tienes GPU NVIDIA, edita `docker-compose.yml`:

```yaml
ollama:
  image: ollama/ollama:latest
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
```

### Configuraci√≥n de Memoria

Para modelos m√°s grandes, ajusta la memoria de Docker:

```yaml
ollama:
  image: ollama/ollama:latest
  deploy:
    resources:
      limits:
        memory: 8G
      reservations:
        memory: 4G
```

## üß™ Pruebas y Verificaci√≥n

### 1. Verificar Estado de Ollama
```bash
# Verificar que Ollama est√© corriendo
curl http://localhost:11434/api/tags

# Ver modelos disponibles
docker-compose exec ollama ollama list
```

### 2. Probar Modelo Local
```bash
# Probar directamente con Ollama
docker-compose exec ollama ollama run phi3:mini "Hello, test message"
```

### 3. Verificar desde la API
```bash
# Probar endpoint de IA local
curl -X POST http://localhost:8000/api/v1/ai/test-local-ai \
  -H "Authorization: Bearer YOUR_TOKEN"
```

### 4. Verificar desde la Interfaz
1. Ve a **Settings** ‚Üí **AI Configuration**
2. Haz clic en **"Test Local AI"**
3. Verifica que aparezca "Local AI Test Successful"

## üîç Monitoreo y Logs

### Ver Logs de Ollama
```bash
docker-compose logs -f ollama
```

### Ver Logs del Backend
```bash
docker-compose logs -f backend
```

### Ver Logs de IA Local
```bash
docker-compose logs -f backend | grep "local_ai"
```

## üö® Soluci√≥n de Problemas

### Problema: Ollama no inicia
```bash
# Verificar que el puerto 11434 est√© libre
netstat -tulpn | grep 11434

# Reiniciar Ollama
docker-compose restart ollama
```

### Problema: Modelos no se descargan
```bash
# Verificar conexi√≥n a internet
docker-compose exec ollama ping google.com

# Descargar manualmente
docker-compose exec ollama ollama pull phi3:mini
```

### Problema: IA local no responde
```bash
# Verificar que los modelos est√©n cargados
docker-compose exec ollama ollama list

# Reiniciar el backend
docker-compose restart backend
```

### Problema: Bajo rendimiento
1. Verifica que tienes suficiente RAM (8GB+)
2. Considera usar un modelo m√°s peque√±o
3. Ajusta `LOCAL_AI_TIMEOUT` si es necesario
4. Verifica que no haya otros procesos consumiendo recursos

## üìä Comparaci√≥n de Rendimiento

| M√©trica | Local AI | OpenAI |
|---------|----------|--------|
| **Latencia** | 1-3 segundos | 2-5 segundos |
| **Costo** | $0 | $0.01-0.10 por an√°lisis |
| **Privacidad** | 100% local | Datos enviados a OpenAI |
| **Disponibilidad** | 99.9% | Depende de OpenAI |
| **Personalizaci√≥n** | Total | Limitada |

## üîÑ Migraci√≥n desde OpenAI

### 1. Configuraci√≥n Gradual
1. Cambia `AI_PROVIDER` a `hybrid`
2. Prueba con ambos proveedores
3. Monitorea el rendimiento
4. Cambia a `local` cuando est√©s seguro

### 2. Configuraci√≥n Inmediata
1. Cambia `AI_PROVIDER` a `local`
2. Inicializa los modelos locales
3. Prueba la funcionalidad
4. Desactiva OpenAI si todo funciona bien

## üìà Optimizaci√≥n

### Para Microtransacciones
- Usa **Phi-3 Mini** o **Gemma 2B**
- Configura `AI_TEMPERATURE=0.1`
- Ajusta `AI_CONFIDENCE_THRESHOLD=0.7`
- Usa `AI_ANALYSIS_INTERVAL=30` segundos

### Para Trading de Medio Plazo
- Usa **Llama 3.1 8B** o **Mistral 7B**
- Configura `AI_TEMPERATURE=0.2`
- Ajusta `AI_CONFIDENCE_THRESHOLD=0.6`
- Usa `AI_ANALYSIS_INTERVAL=60` segundos

## üÜò Soporte

Si encuentras problemas:

1. **Verifica los logs**: `docker-compose logs -f`
2. **Revisa la documentaci√≥n**: Este archivo y el README principal
3. **Prueba con un modelo m√°s peque√±o**: Cambia a Phi-3 Mini
4. **Verifica recursos**: CPU, RAM y almacenamiento
5. **Reinicia servicios**: `docker-compose restart`

## üéâ ¬°Listo!

Con esta configuraci√≥n, tienes un sistema de IA completamente local que:

- ‚úÖ No depende de APIs externas
- ‚úÖ No genera costos por uso
- ‚úÖ Mantiene total privacidad
- ‚úÖ Est√° optimizado para microtransacciones
- ‚úÖ Tiene fallback inteligente
- ‚úÖ Es f√°cil de configurar y mantener

¬°Disfruta de tu sistema de trading con IA local! üöÄ
